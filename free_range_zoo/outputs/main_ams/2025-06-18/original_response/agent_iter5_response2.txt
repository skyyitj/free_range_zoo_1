Here's a policy function designed to tackle the problem:

```python
def single_agent_policy(
    # === Agent Properties ===
    agent_pos: Tuple[float, float],              # Current position of the agent (y, x)
    agent_fire_reduction_power: float,           # How much fire the agent can reduce
    agent_suppressant_num: float,                # Amount of fire suppressant available

    # === Team Information ===
    other_agents_pos: List[Tuple[float, float]], # Positions of all other agents [(y1, x1), (y2, x2), ...]

    # === Fire Task Information ===
    fire_pos: List[Tuple[float, float]],         # Locations of all fires [(y1, x1), (y2, x2), ...]
    fire_levels: List[int],                      # Current intensity level of each fire
    fire_intensities: List[float],               # Current intensity value of each fire task

    # === Task Prioritization ===
    fire_putout_weight: List[float],             # Priority weights for fire suppression tasks
) -> int:
    """
    Choose the optimal fire-fighting task for a single agent.

    Input Parameters:
        Refer to the function signature above.

    Returns:
        int: The index of the selected fire task (0 to num_tasks-1)
    """
    import math

    num_tasks = len(fire_pos)
    best_score = -float('inf')  # Initialize best score to negative infinity
    best_task_idx = 0  # Initialize the index of the best task

    # Define temperature parameters for score component transformations
    distance_temperature = 10.0
    intensity_temperature = 5.0
    reward_temperature = 1.0

    # Iterate over all fire locations to calculate scores
    for task_idx in range(num_tasks):
        # Step 1: Calculate the distance component (Euclidean Distance)
        fire_y, fire_x = fire_pos[task_idx]
        agent_y, agent_x = agent_pos
        distance = math.sqrt((fire_y - agent_y) ** 2 + (fire_x - agent_x) ** 2)
        distance_score = math.exp(-distance / distance_temperature)  # Normalize and invert distance

        # Step 2: Calculate the intensity component
        fire_intensity = fire_intensities[task_idx]
        intensity_score = math.exp(fire_intensity / intensity_temperature)  # Higher intensities are weighted more

        # Step 3: Calculate the reward weight component
        reward_score = math.exp(fire_putout_weight[task_idx] / reward_temperature)

        # Step 4: Combine all scores into a single weighted score
        # Higher is better: prioritize high-reward, close locations with high intensity
        score = reward_score * distance_score * intensity_score

        # Step 5: Update the best task based on the score
        if score > best_score:
            best_score = score
            best_task_idx = task_idx

    return best_task_idx
``` 

### Explanation of the Policy:
1. **Distance Component**: Fires closer to the agent are prioritized using normalized inverse exponential decay (`exp(-distance / temperature)`).
2. **Fire Intensity Component**: Higher-intensity fires are prioritized because they are more critical and likely to spread (`exp(intensity / temperature)`).
3. **Reward Component**: Fires with higher associated rewards (`fire_putout_weight`) are prioritized.
4. **Combination**: The three components are multiplied to form a combined score for each fire task.
5. **Selection**: The fire task with the highest score is selected.

### Key Design Considerations:
- **Normalization & Scalability**: Exponential transformations ensure scores scale smoothly and prioritize depending on distance, intensity, and reward.
- **Temperature Parameters**: Used to control the influence of each component â€” adjust these to change the prioritization strategy.
- **Simplicity**: The policy processes each fire task independently, which ensures it scales well with a larger number of tasks.