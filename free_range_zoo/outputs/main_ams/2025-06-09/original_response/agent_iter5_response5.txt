To enhance the performance based on the metrics provided, let's re-evaluate and adjust different components of the scoring system in the policy function:

1. **Average Rewards**: Maximizing average rewards should focus on more effective fire suppression contributions weighted by the priority weights of the fires.
2. **Average Fire Intensity Change**: This suggests that better control and reduced severity of fires at each step should be achieved; increased effectiveness in targeting high-intensity fires may improve this.
3. **Used Suppressant and Suppressant Efficiency**: Managing suppressant use while maintaining efficiency is crucial. The agents must aim to suppress fires effectively using minimal resources per unit of reduction.

Emphasizing fires with the highest payoff, prioritizing close fires, and increasing the impact of potential effectiveness are important steps. Letâ€™s tune our scoring components and temperature values to strike a better balance between these aspects.

Below is the revised policy function reflecting these considerations:

```python
def single_agent_policy(
    agent_pos: Tuple[float, float], 
    agent_fire_reduction_power: float,
    agent_suppressant_num: float, 
    other_agents_pos: List[Tuple[float, float]], 
    fire_pos: List[Tuple[float, float]], 
    fire_levels: List[int], 
    fire_intensities: List[float],
    fire_putout_weight: List[float]
) -> int:
    num_tasks = len(fire_pos)
    best_task_index = -1
    highest_score = float('-inf')

    # Readjust temperature parameters
    distance_temp = 1.0  # Reassess the impact of distance
    effectiveness_temp = 2.0  # More attention on the capability to reduce the fire
    importance_temp = 3.0  # Further emphasize task importance given the performance influence
    
    for task_index in range(num_tasks):
        fire = fire_pos[task_index]
        distance = np.sqrt((agent_pos[0] - fire[0])**2 + (agent_pos[1] - fire[1])**2)

        # Target usage considering intensities and level contribution, for better resource efficiency
        target_suppressant_use = min(agent_suppressant_num, fire_intensities[task_index] / agent_fire_reduction_power * 0.8)
        potential_effectiveness = agent_fire_reduction_power * target_suppressant_use
      
        # Use task weight for importance scaling
        importance_weight = fire_putout_weight[task_index]
        
        # Score combining components taking into account newly adjusted temperatures
        task_score = (-np.exp(-(distance + 1)) / distance_temp +  # Reduced the distance attenuation
                      np.log(potential_effectiveness + 1) * effectiveness_temp +  # Increased focus on effectiveness
                      importance_weight * 10.0 / importance_temp  # Adjusted importance weight effects
                     )
        
        if task_score > highest_score:
            highest_score = task_score
            best_task_index = task_index

    return best_task_index
```

This version of the policy function attempts to balance reward maximization with efficient suppressant use and impactful fire suppression. Distance impact is adjusted, and effectiveness of suppression is prioritized to align with the main goals derived from the metrics.