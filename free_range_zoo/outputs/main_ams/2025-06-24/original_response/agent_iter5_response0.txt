```python
def single_agent_policy(
    # === Agent Properties ===
    agent_pos: Tuple[float, float],              # Current position of the agent (y, x)
    agent_fire_reduction_power: float,           # How much fire the agent can reduce
    agent_suppressant_num: float,                # Amount of fire suppressant available

    # === Team Information ===
    other_agents_pos: List[Tuple[float, float]], # Positions of all other agents [(y1, x1), (y2, x2), ...]

    # === Fire Task Information ===
    fire_pos: List[Tuple[float, float]],         # Locations of all fires [(y1, x1), (y2, x2), ...]
    fire_levels: List[int],                      # Current intensity level of each fire
    fire_intensities: List[float],               # Current intensity value of each fire task

    # === Task Prioritization ===
    fire_putout_weight: List[float],             # Priority weights for fire suppression tasks
) -> int:
    """
    Choose the optimal fire-fighting task for a single agent.

    Input Parameters:
        Agent Properties:
            agent_pos: (y, x) coordinates of the agent
            agent_fire_reduction_power: Fire suppression capability
            agent_suppressant_num: Available suppressant resources

        Team Information:
            other_agents_pos: List of (y, x) positions for all other agents
                            Shape: (num_agents-1, 2)

        Fire Information:
            fire_pos: List of (y, x) coordinates for all fires
                     Shape: (num_tasks, 2)
            fire_levels: Current fire intensity at each location
                        Shape: (num_tasks,)
            fire_intensities: Base difficulty of extinguishing each fire
                            Shape: (num_tasks,)

        Task Weights:
            fire_putout_weight: Priority weights for task selection
                               Shape: (num_tasks,)

    Returns:
        int: The index of the selected fire task (0 to num_tasks-1)
    """
    # Helper function to compute Manhattan distance
    def manhattan_distance(pos1, pos2):
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])

    num_tasks = len(fire_pos)
    task_scores = []

    # Temperature variables for soft-normalizing score components
    intensity_temp = 1.0
    distance_temp = 10.0
    weight_temp = 1.0
    
    # Iterate over each fire task and calculate its score
    for i in range(num_tasks):
        # Distance between agent and fire location
        distance = manhattan_distance(agent_pos, fire_pos[i])
        
        # Contribution of fire intensity
        normalized_intensity = fire_intensities[i] / max(fire_intensities)
        intensity_score = -normalized_intensity * np.exp(-normalized_intensity / intensity_temp)
        
        # Contribution of distance
        normalized_distance = distance / max(1, sum(manhattan_distance(agent_pos, pos) for pos in fire_pos) / len(fire_pos))
        distance_score = np.exp(-normalized_distance / distance_temp)
        
        # Contribution of fire priority weight
        normalized_weight = fire_putout_weight[i] / max(fire_putout_weight)
        weight_score = normalized_weight * np.exp(normalized_weight / weight_temp)
        
        # Resource management score
        remaining_resource_penalty = (
            (fire_levels[i] - (agent_fire_reduction_power * agent_suppressant_num)) / max(1, fire_levels[i])
        )
        resource_score = -max(remaining_resource_penalty, 0)
        
        # Final task score is a weighted combination
        total_score = intensity_score + distance_score + weight_score + resource_score
        task_scores.append(total_score)

    # Choose the task with the maximum score
    best_task_index = int(np.argmax(task_scores))

    return best_task_index
```