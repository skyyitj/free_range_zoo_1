```python
def single_agent_policy(
    agent_pos: Tuple[float, float], 
    agent_fire_reduction_power: float, 
    agent_suppressant_num: float, 

    other_agents_pos: List[Tuple[float, float]], 

    fire_pos: List[Tuple[float, float]], 
    fire_levels: List[int], 
    fire_intensities: List[float], 

    fire_putout_weight: List[float]
) -> int:
    # === Fire Task Selection Scoring ===
    max_score = float('-inf')
    best_fire = None
    
    for i, (fire_position, fire_level, fire_intensity, fire_weight) in enumerate(zip(fire_pos, fire_levels, fire_intensities, fire_putout_weight)):
        # Calculate euclidean distance from agent to each fire
        dist = ((fire_position[0]-agent_pos[0])**2 + (fire_position[1]-agent_pos[1])**2)**0.5
        
        # Score = Weight × Suppression Potential - Distance × Fire Intensity
        # Higher fire weight and suppression potential increase score, while higher distance and fire intensity decrease it
        # Add a parameter inside np.exp function to scale down the effect of distance
        distance_scale = 0.5
        intensity_scale = 0.5
        
        score = fire_weight * np.exp(-(dist * distance_scale)) * (agent_suppressant_num * agent_fire_reduction_power/fire_intensity) - fire_intensity * intensity_scale
        if score > max_score:
            max_score = score
            best_fire = i

    return best_fire
```
By adding parameters inside the np.exp function we scaled down the effect of distance and fire intensity. This prevents agents from choosing far high intensity fire that they cannot handle effectively, thus allowing them to conserve their resources and enabling them to take on more tasks, decreasing the average burning number and increasing the average putout number. The objectives of this revision are to decrease the average steps and used suppressant while increasing suppressant efficiency and rewards.