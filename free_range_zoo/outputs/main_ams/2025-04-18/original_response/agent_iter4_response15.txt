```python
def single_agent_policy(
    agent_pos: Tuple[float, float], 
    agent_fire_reduction_power: float, 
    agent_suppressant_num: float, 

    other_agents_pos: List[Tuple[float, float]], 

    fire_pos: List[Tuple[float, float]], 
    fire_levels: List[int], 
    fire_intensities: List[float], 

    fire_putout_weight: List[float]
) -> int:

    # === Scoring Fire Tasks ===
    max_score = float('-inf')
    best_fire = None
    distance_task_temperature = 0.1  # introduce a temperature parameter to adjust the soft-max function for distance
    intensity_task_temperature = 0.3  # introduce a temperature parameter to adjust the soft-max function for fire intensity

    for i, (fire_position, fire_level, fire_intensity, fire_weight) in enumerate(zip(fire_pos, fire_levels, fire_intensities, fire_putout_weight)):

        # Introduce an additional reward for extinguishing fires that are not being targeted by other agents
        fire_unhandled = True
        for other_agent_pos in other_agents_pos:
            if fire_position == other_agent_pos:
                fire_unhandled = False
                break

        # Additional reward for extinguishing fires that are unhandled
        unhandled_bonus = 2 * fire_unhandled

        # Distance-based factor for task selection
        dist = ((fire_position[0]-agent_pos[0])**2 + (fire_position[1]-agent_pos[1])**2)**0.5 / (agent_suppressant_num+1)

        # Firefighting efficiency factor in task selection
        suppression_power = agent_fire_reduction_power * agent_suppressant_num / (fire_intensity + 1)

        # Score calculation considering prioritization weight, firefighting efficiency, distance factor and unhandled bonus
        # Include the fire level as an additional factor to prioritize fires with higher levels
        # Introduce a soft-max function with two temperature parameters to the score computation.
        # The first amplifies the differentiation between distances, the second fire intensities
        score = np.exp((fire_weight * suppression_power + unhandled_bonus) / (dist + intensity_task_temperature) + fire_level / distance_task_temperature)

        if score > max_score:
            max_score = score
            best_fire = i

    return best_fire
```

In the revised policy, an additional check is added to reward the agent if it is targeting a fire that is currently not being handled by any other agent. This incentivizes the agents to spread out and handle different fires improving overall performance.

Also, a second temperature parameter, `intensity_task_temperature`, is introduced in the agent's soft-max function. It is there to balance the trade-off between the distance and the fire intensity in the agent's task selection. If the agent is far away but the fire intensity is high, the agent is encouraged to extinguish the fire with a high intensity first and ignore the low-intensity fire. This can prevent fires from outbursts to a larger scale and achieve better results. 

Remember that this policy is subject to hyperparameters (`distance_task_temperature`, `intensity_task_temperature`, and `unhandled_bonus`) that govern the policy's behavior, and fine-tuning is recommended to get optimal results.