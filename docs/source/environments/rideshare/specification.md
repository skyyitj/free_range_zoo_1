---
autogenerated: true
---

# Specification

---

| Import             | `from free_range_zoo.envs import rideshare_v0`                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Actions            | Discrete & Deterministic                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Observations       | Discrete and fully observed with private observations                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Parallel API       | Yes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Manual Control     | No                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Agent Names        | [$driver$_0, ... , $driver$_n]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| # Agents           | $n$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Action Shape       | ($envs$, 2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Action Values      | [$[accept (0)\|pick (1)\|drop (2)]\_0$, ..., $[accept (0)\|pick (1)\|drop (2)]\_{tasks}$, $noop$ (-1)]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Observation Shape  | TensorDict: { <br>&emsp;**self**: $<y, x, num_{accepted}, num_{riding}>$<br>&emsp;**others**: $<y, x, num_{accepted}, num_{riding}>$<br>&emsp;**tasks**: $<y, x, y_{dest}, x_{dest}, accepted_by, riding_by, entered_step>$ <br> **batch_size**: $num\_envs$ }                                                                                                                                                                                                                                                                                                                                                             |
| Observation Values | <u>**self**</u>:<br>&emsp;$y$:$[0, max_y]$<br>&emsp;$x$: $[0, max_x]$<br>&emsp;$num\_accepted$: $[0, pooling\_limit]$<br>&emsp;$num_riding$: $[0, pooling\_limit]$<br><u>**others**</u>:<br>&emsp;$y$:$[0, max_y]$<br>&emsp;$x$: $[0, max_x]$<br>&emsp;$num\_accepted$: $[0, pooling\_limit]$<br>&emsp;$num_riding$: $[0, pooling\_limit]$<u>**tasks**</u>:<br>&emsp;$y$: $[0, max_y]$<br>&emsp;$x$: $[0, max_x]$<br>&emsp;$y_{dest}$: $[0, max_y]$<br>&emsp;$x_{dest}$: $[0, max_x]$<br>&emsp;$riding\_by$: $[0, num_{agents}]$<br>&emsp;$accepted\_by$: $[0, num_{agents}]$<br>&emsp;$entered\_step$: $[0, max_{steps}]$ |

---

## Usage

### Parallel API
```python
from free_range_zoo.envs import rideshare_v0

main_logger = logging.getLogger(__name__)

# Initialize and reset environment to initial state
env = rideshare_v0.parallel_env(render_mode="human")
observations, infos = env.reset()

# Initialize agents and give initial observations
agents = []

cumulative_rewards = {agent: 0 for agent in env.agents}

current_step = 0
while not torch.all(env.finished):
    agent_actions = {
        agent_name: torch.stack([agents[agent_name].act()])
        for agent_name in env.agents
    }  # Policy action determination here

    observations, rewards, terminations, truncations, infos = env.step(agent_actions)
    rewards = {agent_name: rewards[agent_name].item() for agent_name in env.agents}

    for agent_name, agent in agents.items():
        agent.observe(observations[agent_name][0])  # Policy observation processing here
        cumulative_rewards[agent_name] += rewards[agent_name]

    main_logger.info(f"Step {current_step}: {rewards}")
    current_step += 1

env.close()
```

### AEC API
```python
from free_range_zoo.envs import rideshare_v0

main_logger = logging.getLogger(__name__)

# Initialize and reset environment to initial state
env = rideshare_v0.parallel_env(render_mode="human")
observations, infos = env.reset()

# Initialize agents and give initial observations
agents = []

cumulative_rewards = {agent: 0 for agent in env.agents}

current_step = 0
while not torch.all(env.finished):
    for agent in env.agent_iter():
        observations, rewards, terminations, truncations, infos = env.last()

        # Policy action determination here
        action = env.action_space(agent).sample()

        env.step(action)

    rewards = {agent: rewards[agent].item() for agent in env.agents}
    cumulative_rewards[agent] += rewards[agent]

    current_step += 1
    main_logger.info(f"Step {current_step}: {rewards}")

env.close()
```

---

## Configuration

```{eval-rst}
.. currentmodule:: free_range_zoo.envs.rideshare.env.structures.configuration

.. autoclass:: AgentConfiguration
.. autoclass:: PassengerConfiguration
.. autoclass:: RewardConfiguration
.. autoclass:: RideshareConfiguration

```

## API

```{eval-rst}
.. currentmodule:: free_range_zoo.envs.rideshare.env.rideshare

.. autoclass:: env
.. autoclass:: raw_env
```
